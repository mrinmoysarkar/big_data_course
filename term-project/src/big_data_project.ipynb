{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: Mrinmoy sarkar\n",
    "# email: mrinmoy.pol@gmail.com\n",
    "# date: 9/27/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import time  \n",
    "from keras.preprocessing.image import load_img\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# companion functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def unroll(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return img.ravel()\n",
    "\n",
    "class batch_generator():\n",
    "    def __init__(self,X,y):\n",
    "        self.index = 0\n",
    "        self.images = X['images'].apply(unroll).values\n",
    "        self.masks = y['masks'].apply(unroll).values\n",
    "        self.max_index = X.shape[0]\n",
    "        \n",
    "    def get_next_batch(self,batch_size):\n",
    "        if self.index+batch_size <= self.max_index:\n",
    "            x = self.images[self.index:self.index+batch_size]\n",
    "            y = self.masks[self.index:self.index+batch_size]\n",
    "            self.index += batch_size\n",
    "        elif (self.max_index - self.index) > 0:\n",
    "            x = self.images[self.index:self.max_index]\n",
    "            y = self.masks[self.index:self.max_index]\n",
    "            self.index = self.max_index\n",
    "        else:\n",
    "            self.reset_index()\n",
    "            x = self.images[self.index:self.index+batch_size]\n",
    "            y = self.masks[self.index:self.index+batch_size]\n",
    "            self.index += batch_size\n",
    "        return np.array(x.tolist()),np.array(y.tolist())\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        x = self.images[0:self.max_index]\n",
    "        y = self.masks[0:self.max_index]\n",
    "        return np.array(x.tolist()),np.array(y.tolist())\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../data/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"../data/train/images/{}.png\".format(idx), color_mode=\"grayscale\")) / 255 for idx in (train_df.index)]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"../data/train/masks/{}.png\".format(idx), color_mode=\"grayscale\")) / 255 for idx in (train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"images\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some sample image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 20\n",
    "for ind in [1,5,9]:\n",
    "    plt.subplot(3,4,ind)\n",
    "    plt.imshow(train_df[\"images\"][(ind-1)//2+offset],cmap='gray')\n",
    "    plt.subplot(3,4,ind+1)\n",
    "    plt.imshow(train_df[\"masks\"][(ind-1)//2+offset],cmap='gray')\n",
    "    plt.subplot(3,4,ind+2)\n",
    "    plt.imshow(train_df[\"images\"][((ind-1)//2)+1+offset],cmap='gray')\n",
    "    plt.subplot(3,4,ind+3)\n",
    "    plt.imshow(train_df[\"masks\"][((ind-1)//2)+1+offset],cmap='gray')\n",
    "# train_df[\"images\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=train_df[\"images\"],columns=['images'])\n",
    "y = pd.DataFrame(data=train_df[\"masks\"],columns=['masks'])\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)#, random_state=101)\n",
    "batch_gen = batch_generator(X_train,y_train)\n",
    "testbatch_gen = batch_generator(X_test,y_test)\n",
    "total_no_of_train_samples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the DCNN Auto-Encoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.01\n",
    "dilation_rate_r = 1\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "# if True:\n",
    "x = tf.placeholder(tf.float32,shape=[None,img_size_ori*img_size_ori])\n",
    "y_true = tf.placeholder(tf.float32,shape=[None,img_size_ori*img_size_ori])\n",
    "\n",
    "inputs_ori = tf.reshape(x,[-1,img_size_ori,img_size_ori,1])#tf.placeholder(tf.float32,shape=(None,128,128,1),name='inputs')\n",
    "targets_ori = tf.reshape(y_true,[-1,img_size_ori,img_size_ori,1])#tf.placeholder(tf.float32,shape=(None,128,128,1),name='targets')\n",
    "\n",
    "inputs = tf.image.resize_images(inputs_ori,size=(128,128),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "#     targets = tf.image.resize_images(targets_ori,size=(128,128),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=inputs,filters=8,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "maxpool1 = tf.layers.max_pooling2d(inputs=conv1,pool_size=(2,2),strides=(2,2),padding='same')\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=maxpool1,filters=8,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "maxpool2 = tf.layers.max_pooling2d(inputs=conv2,pool_size=(2,2),strides=(2,2),padding='same')\n",
    "\n",
    "conv3 = tf.layers.conv2d(inputs=maxpool2,filters=16,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "maxpool3 = tf.layers.max_pooling2d(inputs=conv3,pool_size=(2,2),strides=(2,2),padding='same')\n",
    "\n",
    "conv4 = tf.layers.conv2d(inputs=maxpool3,filters=16,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "maxpool4 = tf.layers.max_pooling2d(inputs=conv4,pool_size=(2,2),strides=(2,2),padding='same')\n",
    "\n",
    "conv5 = tf.layers.conv2d(inputs=maxpool4,filters=8,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "encoded = tf.layers.max_pooling2d(inputs=conv5,pool_size=(2,2),strides=(2,2),padding='same')\n",
    "\n",
    "\n",
    "upsampler1 = tf.image.resize_images(encoded,size=(8,8),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "conv6 = tf.layers.conv2d(inputs=upsampler1,filters=8,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "\n",
    "upsampler2 = tf.image.resize_images(conv6,size=(16,16),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "conv7 = tf.layers.conv2d(inputs=upsampler2,filters=16,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "\n",
    "upsampler3 = tf.image.resize_images(conv7,size=(32,32),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "conv8 = tf.layers.conv2d(inputs=upsampler3,filters=16,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "\n",
    "upsampler4 = tf.image.resize_images(conv8,size=(64,64),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "conv9 = tf.layers.conv2d(inputs=upsampler4,filters=8,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "\n",
    "upsampler5 = tf.image.resize_images(conv9,size=(128,128),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "conv10 = tf.layers.conv2d(inputs=upsampler5,filters=8,kernel_size=(3,3),dilation_rate=(dilation_rate_r,dilation_rate_r),padding='same',activation=tf.nn.relu)\n",
    "\n",
    "downsampler = tf.image.resize_images(conv10,size=(img_size_ori,img_size_ori),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "logits = tf.layers.conv2d(inputs=downsampler,filters=1,kernel_size=(3,3),padding='same',activation=None)\n",
    "decoded = tf.nn.sigmoid(logits)\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_ori,logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network and calculate Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "epochs = 1001\n",
    "batch_size = 100\n",
    "training_error = []\n",
    "test_error = []\n",
    "no_of_iteration = []\n",
    "loadmodel_fromfile = True\n",
    "iterationNo = 50000\n",
    "model_file_name = \"./tmp/model_\"+str(iterationNo)+\".ckpt\"\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    if loadmodel_fromfile:\n",
    "        saver.restore(sess, model_file_name)\n",
    "    else:\n",
    "        options = tf.RunOptions(output_partition_graphs=True)\n",
    "        metadata = tf.RunMetadata()\n",
    "        sess.run(tf.global_variables_initializer(),options=options, run_metadata=metadata)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        t1 = time.time()\n",
    "        for ii in range(total_no_of_train_samples//batch_size):\n",
    "            x_train_batch,y_train_batch = batch_gen.get_next_batch(batch_size=batch_size)\n",
    "            batch_cost, _ = sess.run([cost,optimizer],feed_dict={x:x_train_batch,y_true:y_train_batch})\n",
    "#             print(\"Trainning loss: {:.4f}\".format(batch_cost))\n",
    "#             print(sess.run([cost,optimizer],feed_dict={x:x_train,y_true:y_train},options=options, run_metadata=metadata))\n",
    "#             print(metadata.partition_graphs)\n",
    "        print(\"epoch No.: \", e, ' time per epoch: ', (time.time()-t1),' batch cost: ', batch_cost)\n",
    "        if e%50 == 0 and e != 0:\n",
    "            tr_loss = 0\n",
    "            te_loss = 0\n",
    "            n = 0\n",
    "            t1 = time.time()\n",
    "            for i in range(2):\n",
    "                for ii in range(total_no_of_train_samples//batch_size):\n",
    "                    xt,yt = batch_gen.get_next_batch(batch_size=batch_size)\n",
    "                    training_loss = sess.run(cost,feed_dict={x:xt,y_true:yt})\n",
    "                    tr_loss += training_loss\n",
    "\n",
    "                    xtest,ytest = testbatch_gen.get_next_batch(batch_size=batch_size)\n",
    "                    test_loss = sess.run(cost,feed_dict={x:xtest,y_true:ytest})\n",
    "                    te_loss += test_loss\n",
    "                    n += 1 \n",
    "            print('training loss: ', tr_loss/n, ' test loss: ', te_loss/n, 'test time: ', time.time()-t1)\n",
    "            training_error.append(tr_loss/n)\n",
    "            test_error.append(te_loss/n)\n",
    "            no_of_iteration.append(e+iterationNo)\n",
    "    save_path = saver.save(sess, \"./tmp/model_\"+str(e+iterationNo)+\".ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)\n",
    "    training_time = time.time()-t\n",
    "    print('training time: ', training_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the training and testing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[no_of_iteration,training_error,test_error]\n",
    "data = np.array(data)\n",
    "data = data.T\n",
    "df = pd.DataFrame(data=data,columns=['no_of_iteration','training_error','test_error'])\n",
    "\n",
    "if loadmodel_fromfile:\n",
    "    df_old = pd.read_csv('trainerror.csv')\n",
    "    concated_pd = pd.concat([df_old,df],axis=0)\n",
    "    concated_pd = concated_pd.reset_index(drop=True)\n",
    "    \n",
    "else:\n",
    "    concated_pd = df\n",
    "\n",
    "concated_pd.to_csv(path_or_buf='trainerror.csv',index=False)\n",
    "\n",
    "no_of_iteration = concated_pd['no_of_iteration']\n",
    "training_error = concated_pd['training_error']\n",
    "test_error = concated_pd['test_error']\n",
    "\n",
    "plt.plot(no_of_iteration,training_error,label=\"Training Loss\")\n",
    "plt.plot(no_of_iteration,test_error,label=\"Test Loss\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"No. of Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict new instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterationNo=51000\n",
    "model_file_name = \"./tmp/model_\"+str(iterationNo)+\".ckpt\"\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    saver.restore(sess, model_file_name)\n",
    "    xtest,ytest = testbatch_gen.get_next_batch(batch_size=6)\n",
    "    print(xtest.shape)\n",
    "    output = sess.run(decoded,feed_dict={x:xtest})\n",
    "    \n",
    "j=1\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "for i in range(6):\n",
    "    y_true= ytest[i]\n",
    "    y_true = y_true.reshape((101,101))\n",
    "    y_pred = output[i].reshape(101,101)\n",
    "    y_pred = y_pred>0.5 # 0.5 is the threshold\n",
    "    plt.subplot(3,4,j)\n",
    "    plt.imshow(y_true,cmap='gray')\n",
    "    plt.title('ground truth')\n",
    "    plt.subplot(3,4,j+1)\n",
    "    plt.imshow(y_pred,cmap='gray')\n",
    "    plt.title('predicted')\n",
    "    j=j+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "for train,test in kf.split(train_df):\n",
    "    train = train_df.iloc[train]\n",
    "    test = train_df.iloc[test]\n",
    "    X_train = pd.DataFrame(data=train[\"images\"],columns=['images'])\n",
    "    y_train = pd.DataFrame(data=train[\"masks\"],columns=['masks'])\n",
    "    X_test =  pd.DataFrame(data=test[\"images\"],columns=['images'])\n",
    "    y_test =  pd.DataFrame(data=test[\"masks\"],columns=['masks'])\n",
    "    batch_gen = batch_generator(X_train,y_train)\n",
    "    testbatch_gen = batch_generator(X_test,y_test)\n",
    "    total_no_of_train_samples = X_train.shape[0]\n",
    "    epochs = 500\n",
    "    batch_size = 100\n",
    "    training_error = []\n",
    "    test_error = []\n",
    "    no_of_iteration = []\n",
    "    loadmodel_fromfile = True\n",
    "    iterationNo = 20000\n",
    "    model_file_name = \"./tmp/model_\"+str(iterationNo)+\".ckpt\"\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if loadmodel_fromfile:\n",
    "            saver.restore(sess, model_file_name)\n",
    "        else:\n",
    "            options = tf.RunOptions(output_partition_graphs=True)\n",
    "            metadata = tf.RunMetadata()\n",
    "            sess.run(tf.global_variables_initializer(),options=options, run_metadata=metadata)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for ii in range(total_no_of_train_samples//batch_size):\n",
    "                x_train_batch,y_train_batch = batch_gen.get_next_batch(batch_size=batch_size)\n",
    "#                 print(x_train_batch.shape)\n",
    "#                 print(y_train_batch.shape)\n",
    "                sess.run([cost,optimizer],feed_dict={x:x_train_batch,y_true:y_train_batch})\n",
    "            \n",
    "        tr_loss = 0\n",
    "        te_loss = 0\n",
    "        n = 0       \n",
    "        for i in range(2):\n",
    "            for ii in range(total_no_of_train_samples//batch_size):\n",
    "                xt,yt = batch_gen.get_next_batch(batch_size=batch_size)\n",
    "                training_loss = sess.run(cost,feed_dict={x:xt,y_true:yt})\n",
    "                tr_loss += training_loss\n",
    "\n",
    "                xtest,ytest = testbatch_gen.get_next_batch(batch_size=batch_size)\n",
    "                test_loss = sess.run(cost,feed_dict={x:xtest,y_true:ytest})\n",
    "                te_loss += test_loss\n",
    "                n += 1 \n",
    "        print('training loss: ', tr_loss/n, ' test loss: ', te_loss/n)\n",
    "        training_error.append(tr_loss/n)\n",
    "        test_error.append(te_loss/n)\n",
    "        \n",
    "print(\"10-fold training loss: \", sum(training_error)/len(training_error), \" and test loss: \",  sum(test_error)/len(test_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
